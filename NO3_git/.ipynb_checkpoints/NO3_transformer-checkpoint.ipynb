{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70003ae5",
   "metadata": {},
   "source": [
    "# Prediction of Key Variables in Wastewater Treatment Plants Using Machine Learning Models\n",
    "\n",
    "## Case Studies - Transformer Algorithm\n",
    "\n",
    "## Notebook developed for WCCI paper case study: <p style=\"color:blue\">Nitrate ($NO_3$).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f930b892",
   "metadata": {},
   "source": [
    "## Data source: Benchmark Simulation Model No 2 - BSM2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3634a4",
   "metadata": {},
   "source": [
    "## Objective: \n",
    "Predict Nitrate and Nitrite value at the output of the aerobic tank."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a19bd20e",
   "metadata": {},
   "source": [
    "## Initial Exploratory Data Analysis\n",
    "\n",
    "The initial exploratory analysis was performed on the main notebook for the case study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90cd902b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "# Load required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "print('Tensorflow version: {}'.format(tf.__version__))\n",
    "plt.style.use('seaborn')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8fac842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets in csv\n",
    "df_entrada = pd.read_csv('datasets/dado5.csv')\n",
    "df_saida = pd.read_csv('datasets/dado8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "311b9ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the S_NO column from the df_saida to df_entrada\n",
    "df_entrada['S_NO_target'] = df_saida['S_NO']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10ff29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete 5 dummie variable\n",
    "df_entrada.drop([\"d1\", \"d2\",\"d3\",\"dp1\",\"dp2\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89314c9",
   "metadata": {},
   "source": [
    "## Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96dad31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df_entrada.filter(['S_NO', 'S_ND', 'X_S', 'S_NO_target', 'X_S'], axis=1)\n",
    "\n",
    "df = df1[0:17280] # 180 days for training and testing\n",
    "val_data = df1[34744:36001] # 10 days (holdout sample) for final test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed4edf8",
   "metadata": {},
   "source": [
    "## Algorithms: Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f570ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_len = 96\n",
    "d_k = 256\n",
    "d_v = 256\n",
    "n_heads = 12\n",
    "ff_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85b809d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S_NO</th>\n",
       "      <th>S_ND</th>\n",
       "      <th>X_S</th>\n",
       "      <th>S_NO_target</th>\n",
       "      <th>X_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2.265460</td>\n",
       "      <td>0.657996</td>\n",
       "      <td>57.810141</td>\n",
       "      <td>9.220979</td>\n",
       "      <td>57.810141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.451755</td>\n",
       "      <td>0.635936</td>\n",
       "      <td>56.250462</td>\n",
       "      <td>9.276799</td>\n",
       "      <td>56.250462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.658756</td>\n",
       "      <td>0.620582</td>\n",
       "      <td>54.871248</td>\n",
       "      <td>9.310110</td>\n",
       "      <td>54.871248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.853949</td>\n",
       "      <td>0.607559</td>\n",
       "      <td>53.689564</td>\n",
       "      <td>9.332799</td>\n",
       "      <td>53.689564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.028143</td>\n",
       "      <td>0.595661</td>\n",
       "      <td>52.625194</td>\n",
       "      <td>9.331921</td>\n",
       "      <td>52.625194</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       S_NO      S_ND        X_S  S_NO_target        X_S\n",
       "0  2.265460  0.657996  57.810141     9.220979  57.810141\n",
       "1  2.451755  0.635936  56.250462     9.276799  56.250462\n",
       "2  2.658756  0.620582  54.871248     9.310110  54.871248\n",
       "3  2.853949  0.607559  53.689564     9.332799  53.689564\n",
       "4  3.028143  0.595661  52.625194     9.331921  52.625194"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0295e3c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scalerT = scaler.fit(df)\n",
    "train_data = scalerT.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97a4adb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train, y_train = [], []\n",
    "for i in range(seq_len, len(train_data)):\n",
    "    X_train.append(train_data[i-seq_len:i]) # Chunks of training data with a length of 128 df-rows\n",
    "    y_train.append(train_data[:, 3][i]) #Value of 4 column (TSS_target) of df-row 128+1\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "21cda481",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Validation data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scalerV = scaler.fit(val_data)\n",
    "val_data = scalerV.transform(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d6b742e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data\n",
    "X_val, y_val = [], []\n",
    "for i in range(seq_len, len(val_data)):\n",
    "    X_val.append(val_data[i-seq_len:i])\n",
    "    y_val.append(val_data[:, 3][i])\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "87c0b732",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape (17184, 96, 5) (17184,)\n",
      "Validation set shape (1161, 96, 5) (1161,)\n"
     ]
    }
   ],
   "source": [
    "print('Training set shape', X_train.shape, y_train.shape)\n",
    "print('Validation set shape', X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93282d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vector(Layer):\n",
    "    def __init__(self, seq_len, **kwargs):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Initialize weights and biases with shape (batch, seq_len)'''\n",
    "        self.weights_linear = self.add_weight(name='weight_linear',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.bias_linear = self.add_weight(name='bias_linear',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.weights_periodic = self.add_weight(name='weight_periodic',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.bias_periodic = self.add_weight(name='bias_periodic',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        '''Calculate linear and periodic time features'''\n",
    "        x = tf.math.reduce_mean(x[:,:,:4], axis=-1) \n",
    "        time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
    "        time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "\n",
    "        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "        time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "        return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)\n",
    "   \n",
    "    def get_config(self): # Needed for saving and loading model with custom layer\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'seq_len': self.seq_len})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e043fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttention(Layer):\n",
    "    def __init__(self, d_k, d_v):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query = Dense(self.d_k, \n",
    "                           input_shape=input_shape, \n",
    "                           kernel_initializer='glorot_uniform', \n",
    "                           bias_initializer='glorot_uniform')\n",
    "\n",
    "        self.key = Dense(self.d_k, \n",
    "                         input_shape=input_shape, \n",
    "                         kernel_initializer='glorot_uniform', \n",
    "                         bias_initializer='glorot_uniform')\n",
    "\n",
    "        self.value = Dense(self.d_v, \n",
    "                           input_shape=input_shape, \n",
    "                           kernel_initializer='glorot_uniform', \n",
    "                           bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        q = self.query(inputs[0])\n",
    "        k = self.key(inputs[1])\n",
    "\n",
    "        attn_weights = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "\n",
    "        v = self.value(inputs[2])\n",
    "        attn_out = tf.matmul(attn_weights, v)\n",
    "        return attn_out    \n",
    "\n",
    "#############################################################################\n",
    "\n",
    "class MultiAttention(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads):\n",
    "        super(MultiAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.attn_heads = list()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        for n in range(self.n_heads):\n",
    "            self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n",
    "\n",
    "        # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 \n",
    "        self.linear = Dense(input_shape[0][-1], \n",
    "                            input_shape=input_shape, \n",
    "                            kernel_initializer='glorot_uniform', \n",
    "                            bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
    "        concat_attn = tf.concat(attn, axis=-1)\n",
    "        multi_linear = self.linear(concat_attn)\n",
    "        return multi_linear   \n",
    "\n",
    "#############################################################################\n",
    "\n",
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.attn_heads = list()\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
    "        self.attn_dropout = Dropout(self.dropout_rate)\n",
    "        self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "        self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n",
    "        # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1] = 7 \n",
    "        self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1) \n",
    "        self.ff_dropout = Dropout(self.dropout_rate)\n",
    "        self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    \n",
    "  \n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        attn_layer = self.attn_multi(inputs)\n",
    "        attn_layer = self.attn_dropout(attn_layer)\n",
    "        attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
    "\n",
    "        ff_layer = self.ff_conv1D_1(attn_layer)\n",
    "        ff_layer = self.ff_conv1D_2(ff_layer)\n",
    "        ff_layer = self.ff_dropout(ff_layer)\n",
    "        ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
    "        return ff_layer \n",
    "\n",
    "    def get_config(self): # Needed for saving and loading model with custom layer\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'d_k': self.d_k,\n",
    "                       'd_v': self.d_v,\n",
    "                       'n_heads': self.n_heads,\n",
    "                       'ff_dim': self.ff_dim,\n",
    "                       'attn_heads': self.attn_heads,\n",
    "                       'dropout_rate': self.dropout_rate})\n",
    "        return config          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e52c7d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 96, 5)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time2_vector (Time2Vector)      (None, 96, 2)        384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 96, 7)        0           input_1[0][0]                    \n",
      "                                                                 time2_vector[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder (Transforme (None, 96, 7)        99114       concatenate[0][0]                \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder_1 (Transfor (None, 96, 7)        99114       transformer_encoder[0][0]        \n",
      "                                                                 transformer_encoder[0][0]        \n",
      "                                                                 transformer_encoder[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder_2 (Transfor (None, 96, 7)        99114       transformer_encoder_1[0][0]      \n",
      "                                                                 transformer_encoder_1[0][0]      \n",
      "                                                                 transformer_encoder_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 96)           0           transformer_encoder_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 96)           0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           6208        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            65          dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 303,999\n",
      "Trainable params: 303,999\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/40\n",
      "537/537 [==============================] - 692s 1s/step - loss: 0.2295 - mae: 0.3331 - mape: 253.4795 - val_loss: 0.0537 - val_mae: 0.1743 - val_mape: 99.0787\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.05371, saving model to Transformer_NO3.hdf5\n",
      "Epoch 2/40\n",
      "537/537 [==============================] - 631s 1s/step - loss: 0.0527 - mae: 0.1761 - mape: 127.2802 - val_loss: 0.0419 - val_mae: 0.1604 - val_mape: 84.4312\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.05371 to 0.04190, saving model to Transformer_NO3.hdf5\n",
      "Epoch 3/40\n",
      "537/537 [==============================] - 629s 1s/step - loss: 0.0414 - mae: 0.1554 - mape: 108.5371 - val_loss: 0.0291 - val_mae: 0.1332 - val_mape: 83.6583\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.04190 to 0.02913, saving model to Transformer_NO3.hdf5\n",
      "Epoch 4/40\n",
      "537/537 [==============================] - 630s 1s/step - loss: 0.0359 - mae: 0.1442 - mape: 110.1098 - val_loss: 0.0184 - val_mae: 0.0974 - val_mape: 54.5140\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.02913 to 0.01837, saving model to Transformer_NO3.hdf5\n",
      "Epoch 5/40\n",
      "537/537 [==============================] - 628s 1s/step - loss: 0.0349 - mae: 0.1409 - mape: 99.9790 - val_loss: 0.0226 - val_mae: 0.1132 - val_mape: 71.1682\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01837\n",
      "Epoch 6/40\n",
      "537/537 [==============================] - 632s 1s/step - loss: 0.0334 - mae: 0.1378 - mape: 88.4529 - val_loss: 0.0275 - val_mae: 0.1249 - val_mape: 64.0476\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01837\n",
      "Epoch 7/40\n",
      "537/537 [==============================] - 632s 1s/step - loss: 0.0304 - mae: 0.1317 - mape: 107.0641 - val_loss: 0.0302 - val_mae: 0.1430 - val_mape: 89.2781\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01837\n",
      "Epoch 8/40\n",
      "537/537 [==============================] - 634s 1s/step - loss: 0.0294 - mae: 0.1280 - mape: 91.0583 - val_loss: 0.0198 - val_mae: 0.1072 - val_mape: 60.5284\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01837\n",
      "Epoch 9/40\n",
      "537/537 [==============================] - 631s 1s/step - loss: 0.0288 - mae: 0.1272 - mape: 92.3880 - val_loss: 0.0182 - val_mae: 0.0969 - val_mape: 64.8294\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01837 to 0.01817, saving model to Transformer_NO3.hdf5\n",
      "Epoch 10/40\n",
      "537/537 [==============================] - 635s 1s/step - loss: 0.0281 - mae: 0.1248 - mape: 71.4517 - val_loss: 0.0158 - val_mae: 0.0937 - val_mape: 57.2846\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01817 to 0.01582, saving model to Transformer_NO3.hdf5\n",
      "Epoch 11/40\n",
      "537/537 [==============================] - 637s 1s/step - loss: 0.0266 - mae: 0.1226 - mape: 87.6477 - val_loss: 0.0199 - val_mae: 0.1092 - val_mape: 79.1623\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.01582\n",
      "Epoch 12/40\n",
      "537/537 [==============================] - 633s 1s/step - loss: 0.0260 - mae: 0.1203 - mape: 97.3970 - val_loss: 0.0234 - val_mae: 0.1177 - val_mape: 93.5883\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01582\n",
      "Epoch 13/40\n",
      "537/537 [==============================] - 634s 1s/step - loss: 0.0263 - mae: 0.1211 - mape: 73.2637 - val_loss: 0.0184 - val_mae: 0.1044 - val_mape: 65.5133\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01582\n",
      "Epoch 14/40\n",
      "537/537 [==============================] - 637s 1s/step - loss: 0.0253 - mae: 0.1200 - mape: 82.4397 - val_loss: 0.0174 - val_mae: 0.0984 - val_mape: 66.5513\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01582\n",
      "Epoch 15/40\n",
      "537/537 [==============================] - 634s 1s/step - loss: 0.0241 - mae: 0.1164 - mape: 88.1717 - val_loss: 0.0189 - val_mae: 0.1027 - val_mape: 65.9906\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.01582\n",
      "Epoch 16/40\n",
      "537/537 [==============================] - 633s 1s/step - loss: 0.0240 - mae: 0.1165 - mape: 91.5666 - val_loss: 0.0208 - val_mae: 0.1003 - val_mape: 53.9059\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.01582\n",
      "Epoch 17/40\n",
      "537/537 [==============================] - 633s 1s/step - loss: 0.0232 - mae: 0.1137 - mape: 74.6965 - val_loss: 0.0167 - val_mae: 0.0982 - val_mape: 62.1473\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.01582\n",
      "Epoch 18/40\n",
      "537/537 [==============================] - 634s 1s/step - loss: 0.0225 - mae: 0.1128 - mape: 79.4500 - val_loss: 0.0167 - val_mae: 0.0959 - val_mape: 63.7951\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.01582\n",
      "Epoch 19/40\n",
      "537/537 [==============================] - 632s 1s/step - loss: 0.0221 - mae: 0.1119 - mape: 81.0785 - val_loss: 0.0192 - val_mae: 0.1026 - val_mape: 62.7146\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.01582\n",
      "Epoch 20/40\n",
      "537/537 [==============================] - 630s 1s/step - loss: 0.0215 - mae: 0.1086 - mape: 74.5259 - val_loss: 0.0149 - val_mae: 0.0907 - val_mape: 56.4333\n",
      "\n",
      "Epoch 00020: val_loss improved from 0.01582 to 0.01486, saving model to Transformer_NO3.hdf5\n",
      "Epoch 21/40\n",
      "537/537 [==============================] - 632s 1s/step - loss: 0.0215 - mae: 0.1102 - mape: 75.7628 - val_loss: 0.0187 - val_mae: 0.0996 - val_mape: 58.8857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00021: val_loss did not improve from 0.01486\n",
      "Epoch 22/40\n",
      "537/537 [==============================] - 630s 1s/step - loss: 0.0201 - mae: 0.1065 - mape: 85.2818 - val_loss: 0.0150 - val_mae: 0.0861 - val_mape: 47.9045\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.01486\n",
      "Epoch 23/40\n",
      "537/537 [==============================] - 631s 1s/step - loss: 0.0198 - mae: 0.1052 - mape: 68.2166 - val_loss: 0.0133 - val_mae: 0.0832 - val_mape: 47.9737\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.01486 to 0.01326, saving model to Transformer_NO3.hdf5\n",
      "Epoch 24/40\n",
      "537/537 [==============================] - 631s 1s/step - loss: 0.0194 - mae: 0.1047 - mape: 81.9512 - val_loss: 0.0138 - val_mae: 0.0856 - val_mape: 45.7169\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.01326\n",
      "Epoch 25/40\n",
      "537/537 [==============================] - 629s 1s/step - loss: 0.0185 - mae: 0.1021 - mape: 86.0759 - val_loss: 0.0186 - val_mae: 0.0976 - val_mape: 63.1049\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.01326\n",
      "Epoch 26/40\n",
      "537/537 [==============================] - 626s 1s/step - loss: 0.0186 - mae: 0.1027 - mape: 78.2169 - val_loss: 0.0165 - val_mae: 0.0952 - val_mape: 64.6266\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.01326\n",
      "Epoch 27/40\n",
      "537/537 [==============================] - 628s 1s/step - loss: 0.0181 - mae: 0.1010 - mape: 78.0433 - val_loss: 0.0152 - val_mae: 0.0905 - val_mape: 56.3219\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.01326\n",
      "Epoch 28/40\n",
      "537/537 [==============================] - 634s 1s/step - loss: 0.0171 - mae: 0.0983 - mape: 92.9749 - val_loss: 0.0154 - val_mae: 0.0904 - val_mape: 50.2197\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.01326\n",
      "Epoch 29/40\n",
      "537/537 [==============================] - 628s 1s/step - loss: 0.0168 - mae: 0.0971 - mape: 82.0042 - val_loss: 0.0136 - val_mae: 0.0845 - val_mape: 40.6189\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.01326\n",
      "Epoch 30/40\n",
      "537/537 [==============================] - 626s 1s/step - loss: 0.0165 - mae: 0.0961 - mape: 62.0942 - val_loss: 0.0148 - val_mae: 0.0895 - val_mape: 49.8909\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.01326\n",
      "Epoch 31/40\n",
      "537/537 [==============================] - 628s 1s/step - loss: 0.0165 - mae: 0.0962 - mape: 83.0922 - val_loss: 0.0144 - val_mae: 0.0843 - val_mape: 37.1158\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.01326\n",
      "Epoch 32/40\n",
      "537/537 [==============================] - 543s 1s/step - loss: 0.0150 - mae: 0.0913 - mape: 86.5559 - val_loss: 0.0158 - val_mae: 0.0969 - val_mape: 51.0089\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.01326\n",
      "Epoch 33/40\n",
      "537/537 [==============================] - 446s 831ms/step - loss: 0.0147 - mae: 0.0912 - mape: 58.2482 - val_loss: 0.0127 - val_mae: 0.0763 - val_mape: 36.5509\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.01326 to 0.01271, saving model to Transformer_NO3.hdf5\n",
      "Epoch 34/40\n",
      "537/537 [==============================] - 457s 851ms/step - loss: 0.0139 - mae: 0.0885 - mape: 77.6929 - val_loss: 0.0090 - val_mae: 0.0676 - val_mape: 34.5002\n",
      "\n",
      "Epoch 00034: val_loss improved from 0.01271 to 0.00896, saving model to Transformer_NO3.hdf5\n",
      "Epoch 35/40\n",
      "537/537 [==============================] - 446s 831ms/step - loss: 0.0143 - mae: 0.0896 - mape: 81.1138 - val_loss: 0.0110 - val_mae: 0.0782 - val_mape: 28.9092\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00896\n",
      "Epoch 36/40\n",
      "537/537 [==============================] - 446s 831ms/step - loss: 0.0132 - mae: 0.0854 - mape: 50.0259 - val_loss: 0.0089 - val_mae: 0.0690 - val_mape: 33.3142\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.00896 to 0.00894, saving model to Transformer_NO3.hdf5\n",
      "Epoch 37/40\n",
      "537/537 [==============================] - 396s 737ms/step - loss: 0.0125 - mae: 0.0839 - mape: 69.5036 - val_loss: 0.0078 - val_mae: 0.0629 - val_mape: 29.4051\n",
      "\n",
      "Epoch 00037: val_loss improved from 0.00894 to 0.00777, saving model to Transformer_NO3.hdf5\n",
      "Epoch 38/40\n",
      "537/537 [==============================] - 246s 458ms/step - loss: 0.0125 - mae: 0.0836 - mape: 54.3755 - val_loss: 0.0096 - val_mae: 0.0740 - val_mape: 31.9594\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00777\n",
      "Epoch 39/40\n",
      "537/537 [==============================] - 247s 460ms/step - loss: 0.0121 - mae: 0.0815 - mape: 64.3905 - val_loss: 0.0078 - val_mae: 0.0600 - val_mape: 32.1110\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00777\n",
      "Epoch 40/40\n",
      "537/537 [==============================] - 247s 461ms/step - loss: 0.0111 - mae: 0.0786 - mape: 48.6572 - val_loss: 0.0104 - val_mae: 0.0804 - val_mape: 33.0790\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00777\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    '''Initialize time and transformer layers'''\n",
    "    time_embedding = Time2Vector(seq_len)\n",
    "    attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "    attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "    attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "\n",
    "    '''Construct model'''\n",
    "    in_seq = Input(shape=(seq_len, 5))\n",
    "    x = time_embedding(in_seq)\n",
    "    x = Concatenate(axis=-1)([in_seq, x])\n",
    "    x = attn_layer1((x, x, x))\n",
    "    x = attn_layer2((x, x, x))\n",
    "    x = attn_layer3((x, x, x))\n",
    "    x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=in_seq, outputs=out)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mape'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "callback = tf.keras.callbacks.ModelCheckpoint('Transformer_NO3.hdf5', \n",
    "                                              monitor='val_loss', \n",
    "                                              save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=40, \n",
    "                    callbacks=[callback],\n",
    "                    validation_data=(X_val, y_val))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40796c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('modelos/Transformer_NO3.hdf5',\n",
    "                                   custom_objects={'Time2Vector': Time2Vector, \n",
    "                                                   'SingleAttention': SingleAttention,\n",
    "                                                   'MultiAttention': MultiAttention,\n",
    "                                                   'TransformerEncoder': TransformerEncoder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a0c0313",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Calculate predictions and metrics'''\n",
    "\n",
    "#Calculate predication for training, validation and test data\n",
    "train_pred = model.predict(X_val)\n",
    "\n",
    "#X_val, y_val\n",
    "#Print evaluation metrics for all datasets\n",
    "train_eval = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "print(' ')\n",
    "print('Evaluation metrics')\n",
    "print('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval[0], train_eval[1], train_eval[2]))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "'''Display results'''\n",
    "\n",
    "fig = plt.figure(figsize=(15,20))\n",
    "st = fig.suptitle(\"Transformer + TimeEmbedding Model\", fontsize=22)\n",
    "st.set_y(0.92)\n",
    "\n",
    "#Plot training data results\n",
    "ax11 = fig.add_subplot(311)\n",
    "ax11.plot(val_data[:, 3], label='NO3_real')\n",
    "ax11.plot(np.arange(seq_len, train_pred.shape[0]+seq_len), train_pred, linewidth=1, label='NO3_pred')\n",
    "ax11.set_title(\"NO3_target\", fontsize=18) \n",
    "ax11.set_xlabel('NO3_target')\n",
    "ax11.set_ylabel('NO3_target')\n",
    "ax11.legend(loc=\"best\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "86352998",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction with trained model\n",
    "previsao_trans = model.predict(X_val)\n",
    "\n",
    "# Copy scaler dimension to do inverse normalization\n",
    "prediction_copies = np.repeat(previsao_trans , val_data.shape[1], axis=-1)\n",
    "previstoT = scalerV.inverse_transform(prediction_copies)[:,3] # Predicted value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "45b7934c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1161,)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previstoT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ca8167a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with original format\n",
    "df1 = df_entrada.filter(['S_NO', 'S_ND', 'X_S', 'S_NO_target', 'X_S'], axis=1)\n",
    "\n",
    "df = df1[0:17280] \n",
    "val_data = df1[34744:36001] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e64d5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with actual and forecast values\n",
    "df2 = val_data.S_NO_target\n",
    "df2 = df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a23db519",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[96:1257]\n",
    "df3 = df3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c35f4c97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1161,)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "ec4d935d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3[96:1161]\n",
    "df4 = df4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9a058122",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevT = previstoT[96:1161]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f1c57734",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NO3 graph\n",
    "plt.figure(figsize = (15, 6))\n",
    "plt.plot(df4) \n",
    "plt.plot(prevT) # em verde \n",
    "plt.title('Transformers', family='Arial', fontsize=14)\n",
    "plt.xlabel('Samples(15 minutes)')\n",
    "plt.ylabel('NO3 Value')\n",
    "plt.legend(['Real Value', 'Predicted value'], loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2eee3249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export forecast values\n",
    "pd.DataFrame(prevT).to_csv('previsao_NO3.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
