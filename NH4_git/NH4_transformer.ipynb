{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c9531c2",
   "metadata": {},
   "source": [
    "# Prediction of Key Variables in Wastewater Treatment Plants Using Machine Learning Models\n",
    "\n",
    "## Case Studies - Transformer Algorithm\n",
    "\n",
    "## Notebook developed for WCCI paper case study: <p style=\"color:blue\">Ammonia and Ammonium ($NH_4$).</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1e75f8",
   "metadata": {},
   "source": [
    "## Data source: Benchmark Simulation Model No 2 - BSM2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b2f003",
   "metadata": {},
   "source": [
    "## Objective: \n",
    "<b> Predict value of ammonia and ammonium ($NH_4$) at the exit of the aerobic tank. The data were collected between tanks 2 and 3, and at the exit of tank 5. <b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d773376",
   "metadata": {},
   "source": [
    "## Initial Exploratory Data Analysis\n",
    "\n",
    "The initial exploratory analysis was performed on the main notebook for the case study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "95e89536",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import os, datetime\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import *\n",
    "from tensorflow.keras.layers import *\n",
    "print('Tensorflow version: {}'.format(tf.__version__))\n",
    "plt.style.use('seaborn')\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "329c9f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets in csv\n",
    "df_entrada = pd.read_csv('dado5.csv')\n",
    "df_saida = pd.read_csv('dado8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ee211ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy the S_NH column from the df_saida to df_entrada\n",
    "df_entrada['S_NH_target'] = df_saida['S_NH']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17f0cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete 5 dummie variable\n",
    "df_entrada.drop([\"d1\", \"d2\",\"d3\",\"dp1\",\"dp2\"], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f148ae",
   "metadata": {},
   "source": [
    "## Split data into training and testing sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db94a2b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_treino = df_entrada[0:17280] # 180 days for training and testing\n",
    "df_teste_final = df_entrada[34936:36001] # 10 days (holdout sample) for final test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2a4c83",
   "metadata": {},
   "source": [
    "## Algorithms: Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3f86adb",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "seq_len = 96\n",
    "d_k = 256\n",
    "d_v = 256\n",
    "n_heads = 12\n",
    "ff_dim = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f47adcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scalerT = scaler.fit(df)\n",
    "train_data = scalerT.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c57755e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "X_train, y_train = [], []\n",
    "for i in range(seq_len, len(train_data)):\n",
    "    X_train.append(train_data[i-seq_len:i]) # Chunks of training data with a length of 128 df-rows\n",
    "    y_train.append(train_data[:, 3][i]) #Value of 4 column (TSS_target) of df-row 128+1\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0b0b7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize Validation data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scalerV = scaler.fit(val_data)\n",
    "val_data = scalerV.transform(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f17d2b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation data\n",
    "X_val, y_val = [], []\n",
    "for i in range(seq_len, len(val_data)):\n",
    "    X_val.append(val_data[i-seq_len:i])\n",
    "    y_val.append(val_data[:, 3][i])\n",
    "X_val, y_val = np.array(X_val), np.array(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8159470b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape (17184, 96, 5) (17184,)\n",
      "Validation set shape (1161, 96, 5) (1161,)\n"
     ]
    }
   ],
   "source": [
    "print('Training set shape', X_train.shape, y_train.shape)\n",
    "print('Validation set shape', X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5d5d0c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Time2Vector(Layer):\n",
    "    def __init__(self, seq_len, **kwargs):\n",
    "        super(Time2Vector, self).__init__()\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        '''Initialize weights and biases with shape (batch, seq_len)'''\n",
    "        self.weights_linear = self.add_weight(name='weight_linear',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.bias_linear = self.add_weight(name='bias_linear',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.weights_periodic = self.add_weight(name='weight_periodic',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "        self.bias_periodic = self.add_weight(name='bias_periodic',\n",
    "                                    shape=(int(self.seq_len),),\n",
    "                                    initializer='uniform',\n",
    "                                    trainable=True)\n",
    "\n",
    "    def call(self, x):\n",
    "        '''Calculate linear and periodic time features'''\n",
    "        x = tf.math.reduce_mean(x[:,:,:4], axis=-1) \n",
    "        time_linear = self.weights_linear * x + self.bias_linear # Linear time feature\n",
    "        time_linear = tf.expand_dims(time_linear, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "\n",
    "        time_periodic = tf.math.sin(tf.multiply(x, self.weights_periodic) + self.bias_periodic)\n",
    "        time_periodic = tf.expand_dims(time_periodic, axis=-1) # Add dimension (batch, seq_len, 1)\n",
    "        return tf.concat([time_linear, time_periodic], axis=-1) # shape = (batch, seq_len, 2)\n",
    "   \n",
    "    def get_config(self): # Needed for saving and loading model with custom layer\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'seq_len': self.seq_len})\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0f26ef3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SingleAttention(Layer):\n",
    "    def __init__(self, d_k, d_v):\n",
    "        super(SingleAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.query = Dense(self.d_k, \n",
    "                           input_shape=input_shape, \n",
    "                           kernel_initializer='glorot_uniform', \n",
    "                           bias_initializer='glorot_uniform')\n",
    "\n",
    "        self.key = Dense(self.d_k, \n",
    "                         input_shape=input_shape, \n",
    "                         kernel_initializer='glorot_uniform', \n",
    "                         bias_initializer='glorot_uniform')\n",
    "\n",
    "        self.value = Dense(self.d_v, \n",
    "                           input_shape=input_shape, \n",
    "                           kernel_initializer='glorot_uniform', \n",
    "                           bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        q = self.query(inputs[0])\n",
    "        k = self.key(inputs[1])\n",
    "\n",
    "        attn_weights = tf.matmul(q, k, transpose_b=True)\n",
    "        attn_weights = tf.map_fn(lambda x: x/np.sqrt(self.d_k), attn_weights)\n",
    "        attn_weights = tf.nn.softmax(attn_weights, axis=-1)\n",
    "\n",
    "        v = self.value(inputs[2])\n",
    "        attn_out = tf.matmul(attn_weights, v)\n",
    "        return attn_out    \n",
    "\n",
    "#############################################################################\n",
    "\n",
    "class MultiAttention(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads):\n",
    "        super(MultiAttention, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.attn_heads = list()\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        for n in range(self.n_heads):\n",
    "            self.attn_heads.append(SingleAttention(self.d_k, self.d_v))  \n",
    "\n",
    "        # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1]=7 \n",
    "        self.linear = Dense(input_shape[0][-1], \n",
    "                            input_shape=input_shape, \n",
    "                            kernel_initializer='glorot_uniform', \n",
    "                            bias_initializer='glorot_uniform')\n",
    "\n",
    "    def call(self, inputs):\n",
    "        attn = [self.attn_heads[i](inputs) for i in range(self.n_heads)]\n",
    "        concat_attn = tf.concat(attn, axis=-1)\n",
    "        multi_linear = self.linear(concat_attn)\n",
    "        return multi_linear   \n",
    "\n",
    "#############################################################################\n",
    "\n",
    "class TransformerEncoder(Layer):\n",
    "    def __init__(self, d_k, d_v, n_heads, ff_dim, dropout=0.1, **kwargs):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "        self.d_k = d_k\n",
    "        self.d_v = d_v\n",
    "        self.n_heads = n_heads\n",
    "        self.ff_dim = ff_dim\n",
    "        self.attn_heads = list()\n",
    "        self.dropout_rate = dropout\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.attn_multi = MultiAttention(self.d_k, self.d_v, self.n_heads)\n",
    "        self.attn_dropout = Dropout(self.dropout_rate)\n",
    "        self.attn_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)\n",
    "\n",
    "        self.ff_conv1D_1 = Conv1D(filters=self.ff_dim, kernel_size=1, activation='relu')\n",
    "        # input_shape[0]=(batch, seq_len, 7), input_shape[0][-1] = 7 \n",
    "        self.ff_conv1D_2 = Conv1D(filters=input_shape[0][-1], kernel_size=1) \n",
    "        self.ff_dropout = Dropout(self.dropout_rate)\n",
    "        self.ff_normalize = LayerNormalization(input_shape=input_shape, epsilon=1e-6)    \n",
    "  \n",
    "    def call(self, inputs): # inputs = (in_seq, in_seq, in_seq)\n",
    "        attn_layer = self.attn_multi(inputs)\n",
    "        attn_layer = self.attn_dropout(attn_layer)\n",
    "        attn_layer = self.attn_normalize(inputs[0] + attn_layer)\n",
    "\n",
    "        ff_layer = self.ff_conv1D_1(attn_layer)\n",
    "        ff_layer = self.ff_conv1D_2(ff_layer)\n",
    "        ff_layer = self.ff_dropout(ff_layer)\n",
    "        ff_layer = self.ff_normalize(inputs[0] + ff_layer)\n",
    "        return ff_layer \n",
    "\n",
    "    def get_config(self): # Needed for saving and loading model with custom layer\n",
    "        config = super().get_config().copy()\n",
    "        config.update({'d_k': self.d_k,\n",
    "                       'd_v': self.d_v,\n",
    "                       'n_heads': self.n_heads,\n",
    "                       'ff_dim': self.ff_dim,\n",
    "                       'attn_heads': self.attn_heads,\n",
    "                       'dropout_rate': self.dropout_rate})\n",
    "        return config          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "abd7f122",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 96, 5)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time2_vector (Time2Vector)      (None, 96, 2)        384         input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 96, 7)        0           input_1[0][0]                    \n",
      "                                                                 time2_vector[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder (Transforme (None, 96, 7)        99114       concatenate[0][0]                \n",
      "                                                                 concatenate[0][0]                \n",
      "                                                                 concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder_1 (Transfor (None, 96, 7)        99114       transformer_encoder[0][0]        \n",
      "                                                                 transformer_encoder[0][0]        \n",
      "                                                                 transformer_encoder[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "transformer_encoder_2 (Transfor (None, 96, 7)        99114       transformer_encoder_1[0][0]      \n",
      "                                                                 transformer_encoder_1[0][0]      \n",
      "                                                                 transformer_encoder_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d (Globa (None, 96)           0           transformer_encoder_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 96)           0           global_average_pooling1d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 64)           6208        dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 64)           0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            65          dropout_1[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 303,999\n",
      "Trainable params: 303,999\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/40\n",
      "537/537 [==============================] - 488s 858ms/step - loss: 0.2843 - mae: 0.2727 - mape: 190.8471 - val_loss: 0.2566 - val_mae: 0.2374 - val_mape: 158.7528\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.25663, saving model to Transformer_NH4.hdf5\n",
      "Epoch 2/40\n",
      "537/537 [==============================] - 457s 851ms/step - loss: 0.0560 - mae: 0.1392 - mape: 169.2580 - val_loss: 0.1303 - val_mae: 0.1475 - val_mape: 92.6641\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.25663 to 0.13029, saving model to Transformer_NH4.hdf5\n",
      "Epoch 3/40\n",
      "537/537 [==============================] - 517s 963ms/step - loss: 0.0322 - mae: 0.1100 - mape: 109.3580 - val_loss: 0.0376 - val_mae: 0.1023 - val_mape: 103.8041\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.13029 to 0.03759, saving model to Transformer_NH4.hdf5\n",
      "Epoch 4/40\n",
      "537/537 [==============================] - 617s 1s/step - loss: 0.0300 - mae: 0.1057 - mape: 118.9785 - val_loss: 0.0168 - val_mae: 0.0790 - val_mape: 62.0333\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.03759 to 0.01684, saving model to Transformer_NH4.hdf5\n",
      "Epoch 5/40\n",
      "537/537 [==============================] - 621s 1s/step - loss: 0.0234 - mae: 0.0937 - mape: 119.5110 - val_loss: 0.0247 - val_mae: 0.0916 - val_mape: 75.1492\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 0.01684\n",
      "Epoch 6/40\n",
      "537/537 [==============================] - 622s 1s/step - loss: 0.0204 - mae: 0.0876 - mape: 82.5259 - val_loss: 0.0512 - val_mae: 0.1094 - val_mape: 55.8306\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 0.01684\n",
      "Epoch 7/40\n",
      "537/537 [==============================] - 621s 1s/step - loss: 0.0197 - mae: 0.0863 - mape: 66.6798 - val_loss: 0.0198 - val_mae: 0.0756 - val_mape: 46.1849\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 0.01684\n",
      "Epoch 8/40\n",
      "537/537 [==============================] - 624s 1s/step - loss: 0.0214 - mae: 0.0889 - mape: 79.8359 - val_loss: 0.0185 - val_mae: 0.0965 - val_mape: 105.1004\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 0.01684\n",
      "Epoch 9/40\n",
      "537/537 [==============================] - 622s 1s/step - loss: 0.0160 - mae: 0.0786 - mape: 72.4679 - val_loss: 0.0133 - val_mae: 0.0634 - val_mape: 42.4456\n",
      "\n",
      "Epoch 00009: val_loss improved from 0.01684 to 0.01329, saving model to Transformer_NH4.hdf5\n",
      "Epoch 10/40\n",
      "537/537 [==============================] - 623s 1s/step - loss: 0.0165 - mae: 0.0776 - mape: 73.4695 - val_loss: 0.0108 - val_mae: 0.0656 - val_mape: 70.3760\n",
      "\n",
      "Epoch 00010: val_loss improved from 0.01329 to 0.01075, saving model to Transformer_NH4.hdf5\n",
      "Epoch 11/40\n",
      "537/537 [==============================] - 626s 1s/step - loss: 0.0153 - mae: 0.0747 - mape: 68.3174 - val_loss: 0.0107 - val_mae: 0.0677 - val_mape: 70.1336\n",
      "\n",
      "Epoch 00011: val_loss improved from 0.01075 to 0.01067, saving model to Transformer_NH4.hdf5\n",
      "Epoch 12/40\n",
      "537/537 [==============================] - 626s 1s/step - loss: 0.0140 - mae: 0.0704 - mape: 59.0295 - val_loss: 0.0174 - val_mae: 0.0903 - val_mape: 64.6278\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.01067\n",
      "Epoch 13/40\n",
      "537/537 [==============================] - 623s 1s/step - loss: 0.0175 - mae: 0.0765 - mape: 84.8114 - val_loss: 0.0208 - val_mae: 0.1012 - val_mape: 84.6508\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.01067\n",
      "Epoch 14/40\n",
      "537/537 [==============================] - 624s 1s/step - loss: 0.0158 - mae: 0.0738 - mape: 87.6101 - val_loss: 0.0157 - val_mae: 0.0792 - val_mape: 93.3416\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.01067\n",
      "Epoch 15/40\n",
      "537/537 [==============================] - 623s 1s/step - loss: 0.0140 - mae: 0.0684 - mape: 72.2059 - val_loss: 0.0071 - val_mae: 0.0567 - val_mape: 56.8696\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.01067 to 0.00706, saving model to Transformer_NH4.hdf5\n",
      "Epoch 16/40\n",
      "537/537 [==============================] - 624s 1s/step - loss: 0.0152 - mae: 0.0716 - mape: 58.5636 - val_loss: 0.0156 - val_mae: 0.0683 - val_mape: 24.1536\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.00706\n",
      "Epoch 17/40\n",
      "537/537 [==============================] - 624s 1s/step - loss: 0.0125 - mae: 0.0664 - mape: 60.8872 - val_loss: 0.0075 - val_mae: 0.0584 - val_mape: 63.1435\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.00706\n",
      "Epoch 18/40\n",
      "537/537 [==============================] - 620s 1s/step - loss: 0.0124 - mae: 0.0670 - mape: 52.3040 - val_loss: 0.0114 - val_mae: 0.0722 - val_mape: 72.4720\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.00706\n",
      "Epoch 19/40\n",
      "537/537 [==============================] - 622s 1s/step - loss: 0.0113 - mae: 0.0646 - mape: 50.1530 - val_loss: 0.0052 - val_mae: 0.0423 - val_mape: 33.2417\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.00706 to 0.00518, saving model to Transformer_NH4.hdf5\n",
      "Epoch 20/40\n",
      "537/537 [==============================] - 625s 1s/step - loss: 0.0107 - mae: 0.0617 - mape: 77.8295 - val_loss: 0.0056 - val_mae: 0.0513 - val_mape: 55.6268\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.00518\n",
      "Epoch 21/40\n",
      "537/537 [==============================] - 622s 1s/step - loss: 0.0112 - mae: 0.0636 - mape: 75.6413 - val_loss: 0.0065 - val_mae: 0.0487 - val_mape: 35.9646\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.00518\n",
      "Epoch 22/40\n",
      "537/537 [==============================] - 622s 1s/step - loss: 0.0112 - mae: 0.0639 - mape: 62.6200 - val_loss: 0.0134 - val_mae: 0.0864 - val_mape: 87.9361\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.00518\n",
      "Epoch 23/40\n",
      "537/537 [==============================] - 619s 1s/step - loss: 0.0113 - mae: 0.0622 - mape: 70.1580 - val_loss: 0.0053 - val_mae: 0.0452 - val_mape: 22.9984\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.00518\n",
      "Epoch 24/40\n",
      "537/537 [==============================] - 620s 1s/step - loss: 0.0111 - mae: 0.0625 - mape: 64.8751 - val_loss: 0.0082 - val_mae: 0.0747 - val_mape: 53.7147\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.00518\n",
      "Epoch 25/40\n",
      "537/537 [==============================] - 620s 1s/step - loss: 0.0117 - mae: 0.0628 - mape: 43.7291 - val_loss: 0.0091 - val_mae: 0.0677 - val_mape: 55.9275\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.00518\n",
      "Epoch 26/40\n",
      "537/537 [==============================] - 623s 1s/step - loss: 0.0109 - mae: 0.0602 - mape: 57.8953 - val_loss: 0.0080 - val_mae: 0.0557 - val_mape: 29.6980\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.00518\n",
      "Epoch 27/40\n",
      "537/537 [==============================] - 621s 1s/step - loss: 0.0120 - mae: 0.0633 - mape: 69.1374 - val_loss: 0.0099 - val_mae: 0.0497 - val_mape: 21.2519\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.00518\n",
      "Epoch 28/40\n",
      "537/537 [==============================] - 620s 1s/step - loss: 0.0117 - mae: 0.0621 - mape: 62.1899 - val_loss: 0.0067 - val_mae: 0.0450 - val_mape: 19.8728\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.00518\n",
      "Epoch 29/40\n",
      "537/537 [==============================] - 620s 1s/step - loss: 0.0124 - mae: 0.0644 - mape: 58.7820 - val_loss: 0.0042 - val_mae: 0.0406 - val_mape: 25.4298\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.00518 to 0.00424, saving model to Transformer_NH4.hdf5\n",
      "Epoch 30/40\n",
      "537/537 [==============================] - 620s 1s/step - loss: 0.0100 - mae: 0.0597 - mape: 69.2456 - val_loss: 0.0048 - val_mae: 0.0470 - val_mape: 39.6912\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.00424\n",
      "Epoch 31/40\n",
      "537/537 [==============================] - 618s 1s/step - loss: 0.0094 - mae: 0.0557 - mape: 39.3358 - val_loss: 0.0073 - val_mae: 0.0455 - val_mape: 23.3038\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 0.00424\n",
      "Epoch 32/40\n",
      "537/537 [==============================] - 621s 1s/step - loss: 0.0099 - mae: 0.0578 - mape: 51.5216 - val_loss: 0.0096 - val_mae: 0.0512 - val_mape: 23.8161\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.00424\n",
      "Epoch 33/40\n",
      "537/537 [==============================] - 622s 1s/step - loss: 0.0108 - mae: 0.0589 - mape: 39.7123 - val_loss: 0.0031 - val_mae: 0.0324 - val_mape: 19.8864\n",
      "\n",
      "Epoch 00033: val_loss improved from 0.00424 to 0.00312, saving model to Transformer_NH4.hdf5\n",
      "Epoch 34/40\n",
      "537/537 [==============================] - 616s 1s/step - loss: 0.0095 - mae: 0.0561 - mape: 59.5761 - val_loss: 0.0062 - val_mae: 0.0484 - val_mape: 21.2927\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.00312\n",
      "Epoch 35/40\n",
      "537/537 [==============================] - 575s 1s/step - loss: 0.0095 - mae: 0.0559 - mape: 71.2533 - val_loss: 0.0063 - val_mae: 0.0508 - val_mape: 34.3758\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.00312\n",
      "Epoch 36/40\n",
      "537/537 [==============================] - 442s 822ms/step - loss: 0.0094 - mae: 0.0579 - mape: 48.5229 - val_loss: 0.0034 - val_mae: 0.0354 - val_mape: 19.7582\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 0.00312\n",
      "Epoch 37/40\n",
      "537/537 [==============================] - 442s 824ms/step - loss: 0.0088 - mae: 0.0543 - mape: 33.7588 - val_loss: 0.0056 - val_mae: 0.0564 - val_mape: 35.1727\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.00312\n",
      "Epoch 38/40\n",
      "537/537 [==============================] - 450s 839ms/step - loss: 0.0099 - mae: 0.0567 - mape: 48.1336 - val_loss: 0.0037 - val_mae: 0.0344 - val_mape: 23.1994\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.00312\n",
      "Epoch 39/40\n",
      "537/537 [==============================] - 445s 828ms/step - loss: 0.0109 - mae: 0.0607 - mape: 44.4734 - val_loss: 0.0072 - val_mae: 0.0568 - val_mape: 50.2853\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 0.00312\n",
      "Epoch 40/40\n",
      "537/537 [==============================] - 444s 826ms/step - loss: 0.0110 - mae: 0.0597 - mape: 44.2928 - val_loss: 0.0064 - val_mae: 0.0526 - val_mape: 47.7702\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.00312\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    '''Initialize time and transformer layers'''\n",
    "    time_embedding = Time2Vector(seq_len)\n",
    "    attn_layer1 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "    attn_layer2 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "    attn_layer3 = TransformerEncoder(d_k, d_v, n_heads, ff_dim)\n",
    "\n",
    "    '''Construct model'''\n",
    "    in_seq = Input(shape=(seq_len, 5))\n",
    "    x = time_embedding(in_seq)\n",
    "    x = Concatenate(axis=-1)([in_seq, x])\n",
    "    x = attn_layer1((x, x, x))\n",
    "    x = attn_layer2((x, x, x))\n",
    "    x = attn_layer3((x, x, x))\n",
    "    x = GlobalAveragePooling1D(data_format='channels_first')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    out = Dense(1, activation='linear')(x)\n",
    "\n",
    "    model = Model(inputs=in_seq, outputs=out)\n",
    "    model.compile(loss='mse', optimizer='adam', metrics=['mae', 'mape'])\n",
    "    return model\n",
    "\n",
    "\n",
    "model = create_model()\n",
    "model.summary()\n",
    "\n",
    "callback = tf.keras.callbacks.ModelCheckpoint('Transformer_NH4.hdf5', \n",
    "                                              monitor='val_loss', \n",
    "                                              save_best_only=True, verbose=1)\n",
    "\n",
    "history = model.fit(X_train, y_train, \n",
    "                    batch_size=batch_size, \n",
    "                    epochs=40, \n",
    "                    callbacks=[callback],\n",
    "                    validation_data=(X_val, y_val))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3517f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('Transformer_NH4.hdf5',\n",
    "                                   custom_objects={'Time2Vector': Time2Vector, \n",
    "                                                   'SingleAttention': SingleAttention,\n",
    "                                                   'MultiAttention': MultiAttention,\n",
    "                                                   'TransformerEncoder': TransformerEncoder})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b49f3a0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Calculate predictions and metrics'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Calculate predictions and metrics'''\n",
    "\n",
    "#Calculate predication for training, validation and test data\n",
    "train_pred = model.predict(X_val)\n",
    "\n",
    "#X_val, y_val\n",
    "#Print evaluation metrics for all datasets\n",
    "train_eval = model.evaluate(X_val, y_val, verbose=0)\n",
    "\n",
    "print(' ')\n",
    "print('Evaluation metrics')\n",
    "print('Training Data - Loss: {:.4f}, MAE: {:.4f}, MAPE: {:.4f}'.format(train_eval[0], train_eval[1], train_eval[2]))\n",
    "\n",
    "\n",
    "###############################################################################\n",
    "'''Display results'''\n",
    "\n",
    "fig = plt.figure(figsize=(15,20))\n",
    "st = fig.suptitle(\"Transformer + TimeEmbedding Model\", fontsize=22)\n",
    "st.set_y(0.92)\n",
    "\n",
    "#Plot training data results\n",
    "ax11 = fig.add_subplot(311)\n",
    "ax11.plot(val_data[:, 3], label='S_NH4_target')\n",
    "ax11.plot(np.arange(seq_len, train_pred.shape[0]+seq_len), train_pred, linewidth=1, label='S_NH4_pred')\n",
    "ax11.set_title(\"S_NH4_target\", fontsize=18)\n",
    "ax11.set_xlabel('S_NH4_target') \n",
    "ax11.set_ylabel('S_NH4_target')\n",
    "ax11.legend(loc=\"best\", fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "973bd1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make prediction with trained model\n",
    "previsao_trans = model.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c2160bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy scaler dimension to do inverse normalization\n",
    "prediction_copies = np.repeat(previsao_trans , val_data.shape[1], axis=-1)\n",
    "previstoT = scalerV.inverse_transform(prediction_copies)[:,3] # valor previsto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "98bfa5e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1161,)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previstoT.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8379a6e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data with original format\n",
    "df1 = df_entrada.filter(['S_alk', 'S_NH', 'X_ND', 'S_NH_target', 'X_S'], axis=1)\n",
    "\n",
    "df = df1[0:17280]\n",
    "val_data = df1[34744:36001]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "96007181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with actual and forecast values\n",
    "df2 = val_data.S_NH_target\n",
    "df2 = df2.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "28630e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = df2[96:1257]\n",
    "df3 = df3.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a6229761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1161,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac33aaa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3[96:1161]\n",
    "df4 = df4.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "391fb767",
   "metadata": {},
   "outputs": [],
   "source": [
    "prevT = previstoT[96:1161]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4d12e584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export forecast values\n",
    "pd.DataFrame(prevT).to_csv('previsao_NH4.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
